{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzwKs2jNS3Zw2MuA2PdN1C"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/573071/step/8\n",
        "\n",
        "[Функция Softmax и ее производная](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)\n",
        "\n",
        "[Softmax для нейронных сетей](https://e2eml.school/softmax)\n",
        "\n",
        "Рассмотрим тренировочную выборку\n",
        "\n",
        "| x | y |\n",
        "|---|---|\n",
        "| -1 | 0 |\n",
        "| 1 | 1 |\n",
        "\n",
        "и будем тренировать нейронную сеть со следующей архитектурой:\n",
        "\n",
        "<img src='https://ucarecdn.com/acad3c70-9b9d-4837-9088-c2dca29dae0f/'>\n",
        "\n",
        "Составьте функцию потерь по заданной тренировочной выборке. Там должно быть три слагаемых (по количеству объектов из тренировочной выборки). Для простоты дифференцирования функцию потерь преобразуйте к виду (тут надо пользоваться свойствами логарифмов и почленным делением дробей):\n",
        "\n",
        "$$L=-ln(...)-ln(...)-ln(...)=ln(1+e...)+ln(1+e...)+ln(1+e...)$$\n",
        "\n",
        "Теперь найдите частные производные по всем переменным функции потерь (пользуясь школьным правилами дифференцирования).\n",
        "\n",
        "Прекрасно. А теперь зададим параметры градиентного спуска. Будем считать, что изначально все веса равны $0$, шаг обучения равен $h=0.1$ Найдите новые значения весов **после одного шага** градиентного спуска.  \n",
        "\n",
        "В окошко запишите новое значение веса $w_3$ или веса $w_4$ (если вы все сделали правильно, то значения этих весов будут одинаковыми)."
      ],
      "metadata": {
        "id": "osU2Mwu3zulH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0"
      ],
      "metadata": {
        "id": "87fKmW9Oz6iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Решение\n"
      ],
      "metadata": {
        "id": "D9SNCZLQ0ROM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4V_UrDgzn48",
        "outputId": "2c80302c-7e71-4ae9-fce1-3cd3374c7838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 1, w1 = 0.1000, w2 = -0.1000, w3 = 0.0000, w4 = 0.0000\n",
            "epoch = 2, w1 = 0.1900, w2 = -0.1900, w3 = -0.0000, w4 = -0.0000\n",
            "epoch = 3, w1 = 0.2713, w2 = -0.2713, w3 = -0.0000, w4 = -0.0000\n"
          ]
        }
      ],
      "source": [
        "from math import exp\n",
        "\n",
        "# Функция для вычисления значений до softmax\n",
        "x_raw = lambda x: [w[0] * x + w[2], w[1] * x + w[3]]\n",
        "\n",
        "# Функция softmax\n",
        "def softmax(x):\n",
        "    exp_x = [exp(xi) for xi in x]\n",
        "    sum_exp_x = sum(exp_x)\n",
        "    return [xi / sum_exp_x for xi in exp_x]\n",
        "\n",
        "# Входные данные\n",
        "x = [-1, 1]\n",
        "y = [0, 1]\n",
        "# Начальные значения весов\n",
        "w = [0, 0, 0, 0]\n",
        "# Количество эпох\n",
        "epoches = 3\n",
        "# Шаг обучения\n",
        "h = 0.1\n",
        "\n",
        "# Обучение\n",
        "for epoch in range(epoches):\n",
        "    # Обнуляем градиенты для каждой эпохи\n",
        "    w_grad = [0, 0, 0, 0]\n",
        "    for i in range(len(x)):\n",
        "        # Вычисляем предсказание\n",
        "        y_pred = softmax(x_raw(x[i]))\n",
        "\n",
        "        # Обновляем градиенты для каждого веса\n",
        "        w_grad[0] += (y_pred[0] - y[i]) * x[i]\n",
        "        w_grad[1] += (y_pred[1] - (1-y[i])) * x[i] # One-Hot Encoding для y\n",
        "        w_grad[2] += (y_pred[0] - y[i])\n",
        "        w_grad[3] += (y_pred[1] - (1-y[i])) # One-Hot Encoding для y\n",
        "\n",
        "    # Обновляем веса\n",
        "    w[0] -= h * w_grad[0]\n",
        "    w[1] -= h * w_grad[1]\n",
        "    w[2] -= h * w_grad[2]\n",
        "    w[3] -= h * w_grad[3]\n",
        "\n",
        "    # Выводим веса на каждой эпохе\n",
        "    print(f'epoch = {epoch+1}, w1 = {w[0]:.4f}, w2 = {w[1]:.4f}, w3 = {w[2]:.4f}, w4 = {w[3]:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Форум"
      ],
      "metadata": {
        "id": "x-vFoDXTXepJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, -1], [1, 1]]) # Матрица признаков с дополнительным столбцом из единиц.\n",
        "Y = np.array([[1, 0], [0, 1]])  # Матрица меток в формате One Hot Encoding.\n",
        "W = np.zeros((2, 2)) # Матрица весов. Первый столбец - смещения, второй - веса связей.\n",
        "\n",
        "def softmax(X):\n",
        "    return np.exp(X) / np.exp(X).sum(axis=1, keepdims=True)\n",
        "\n",
        "h = 0.1\n",
        "for i in range(3):\n",
        "    Y_pred = softmax(X @ W.T)   # Вычислить предсказание сети.\n",
        "    grad_W = (Y_pred - Y).T @ X # Вычислить градиент функции потерь по весам.\n",
        "    W -= grad_W * h             # Обновить веса сети.\n",
        "    print('epoch =', i+1, W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu9RbeZ40X1L",
        "outputId": "d856c9ad-6dcb-428a-d1f9-3399cca69df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 1 [[ 0.  -0.1]\n",
            " [ 0.   0.1]]\n",
            "epoch = 2 [[-5.55111512e-18 -1.90033201e-01]\n",
            " [-5.55111512e-18  1.90033201e-01]]\n",
            "epoch = 3 [[-5.55111512e-18 -2.71255377e-01]\n",
            " [-5.55111512e-18  2.71255377e-01]]\n"
          ]
        }
      ]
    }
  ]
}