{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+Is5EjAZXZbg1F2FOgbmf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ответ 1\n",
        "Разберём шаг за шагом, как получается, что производная функции потерь по $ Z_{ik} $ равна $ Y_{\\text{pred},ik} - Y_{ik} $. Здесь мы рассмотрим производную кросс-энтропийной функции потерь в комбинации с softmax-функцией.\n",
        "\n",
        "### Шаги преобразования\n",
        "\n",
        "1. **Вспомним определение softmax-функции** для компоненты $ k $ вектора предсказаний для объекта $ i $:\n",
        "   $$\n",
        "   Y_{\\text{pred},ik} = \\frac{e^{Z_{ik}}}{\\sum_{j} e^{Z_{ij}}}\n",
        "   $$\n",
        "   Здесь $ Z_{ik} $ — это линейное преобразование для класса $ k $ и объекта $ i $, то есть $ Z_{ik} = W_k \\cdot X_i $.\n",
        "\n",
        "2. **Кросс-энтропийная функция потерь** для одного объекта $ i $ выглядит следующим образом:\n",
        "   $$\n",
        "   L_i = -\\sum_{k} Y_{ik} \\log(Y_{\\text{pred},ik})\n",
        "   $$\n",
        "   где $ Y_{ik} $ — истинная метка для класса $ k $ (равна 1 для правильного класса и 0 для остальных классов).\n",
        "\n",
        "3. **Вычислим производную кросс-энтропии по $ Z_{ik} $**:\n",
        "   Чтобы упростить вычисление градиента, мы сначала найдём производную функции потерь по сырым выходам $ Z_{ik} $.\n",
        "\n",
        "   Для этого воспользуемся правилом дифференцирования составной функции (цепное правило). Мы начнём с производной $ L_i $ по $ Y_{\\text{pred},ik} $, а затем будем дифференцировать $ Y_{\\text{pred},ik} $ по $ Z_{ik} $.\n",
        "\n",
        "### Производная кросс-энтропии по выходу softmax\n",
        "\n",
        "Производная функции потерь по $ Y_{\\text{pred},ik} $:\n",
        "   $$\n",
        "   \\frac{\\partial L_i}{\\partial Y_{\\text{pred},ik}} = -\\frac{Y_{ik}}{Y_{\\text{pred},ik}}\n",
        "   $$\n",
        "\n",
        "### Производная softmax-функции по $ Z_{ik} $\n",
        "\n",
        "Теперь найдём производную $ Y_{\\text{pred},ik} $ по $ Z_{ij} $, где $ i $ — это индекс объекта, $ j $ и $ k $ — индексы классов. Здесь нужно учитывать два случая: когда $ j = k $ и когда $ j \\neq k $.\n",
        "\n",
        "1. **Если $ j = k $:**\n",
        "   $$\n",
        "   \\frac{\\partial Y_{\\text{pred},ik}}{\\partial Z_{ik}} = Y_{\\text{pred},ik} (1 - Y_{\\text{pred},ik})\n",
        "   $$\n",
        "\n",
        "2. **Если $ j \\neq k $:**\n",
        "   $$\n",
        "   \\frac{\\partial Y_{\\text{pred},ik}}{\\partial Z_{ij}} = -Y_{\\text{pred},ik} Y_{\\text{pred},ij}\n",
        "   $$\n",
        "\n",
        "### Применение цепного правила\n",
        "\n",
        "Теперь мы можем применить цепное правило, чтобы найти $\\frac{\\partial L_i}{\\partial Z_{ik}}$:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = \\sum_j \\frac{\\partial L_i}{\\partial Y_{\\text{pred},ij}} \\cdot \\frac{\\partial Y_{\\text{pred},ij}}{\\partial Z_{ik}}\n",
        "$$\n",
        "\n",
        "Подставляя значения производных, получаем:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = \\frac{\\partial L_i}{\\partial Y_{\\text{pred},ik}} \\cdot \\frac{\\partial Y_{\\text{pred},ik}}{\\partial Z_{ik}} + \\sum_{j \\neq k} \\frac{\\partial L_i}{\\partial Y_{\\text{pred},ij}} \\cdot \\frac{\\partial Y_{\\text{pred},ij}}{\\partial Z_{ik}}\n",
        "$$\n",
        "\n",
        "Подставляя значения для $\\frac{\\partial L_i}{\\partial Y_{\\text{pred},ik}}$ и $\\frac{\\partial Y_{\\text{pred},ik}}{\\partial Z_{ik}}$, и учитывая выражения выше, мы приходим к следующему результату:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = Y_{\\text{pred},ik} - Y_{ik}\n",
        "$$\n",
        "\n",
        "Таким образом, производная функции потерь по $ Z_{ik} $ оказывается равна разности предсказанной вероятности и истинной метки для класса $ k $ и объекта $ i $, что упрощает вычисление градиента для всей модели."
      ],
      "metadata": {
        "id": "MRyLDBmxwAQg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfuVAFd1qPZ5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ответ 2\n",
        "\n",
        "Конечно, давайте подробно разберём, как выводится производная функции потерь по сырым выходам $ Z $, чтобы стало понятно, почему она сводится к $ Y_{\\text{pred},ik} - Y_{ik} $.\n",
        "\n",
        "Для этого воспользуемся несколькими свойствами производной и рассмотрим, как именно происходит это упрощение.\n",
        "\n",
        "### Повторим обозначения\n",
        "- $ Y $ — истинные метки (One Hot Encoding).\n",
        "- $ Y_{\\text{pred}} $ — предсказанные вероятности для каждого класса, полученные через softmax.\n",
        "- $ Z $ — сырые выходы линейного слоя перед применением softmax.\n",
        "\n",
        "Напомним, что:\n",
        "$$\n",
        "Y_{\\text{pred},ik} = \\frac{e^{Z_{ik}}}{\\sum_{j} e^{Z_{ij}}}\n",
        "$$\n",
        "где $ Y_{\\text{pred},ik} $ — это предсказанная вероятность для $ i $-го объекта и $ k $-го класса, а $ Z_{ik} $ — линейное преобразование (логит) для этого объекта и класса.\n",
        "\n",
        "### Кросс-энтропийная функция потерь\n",
        "Для одного объекта с индексом $ i $ кросс-энтропийная функция потерь задаётся как:\n",
        "$$\n",
        "L_i = -\\sum_{k} Y_{ik} \\log(Y_{\\text{pred},ik})\n",
        "$$\n",
        "Наша цель — найти производную этой функции по $ Z_{ik} $.\n",
        "\n",
        "### Шаг 1: Производная потерь по $ Y_{\\text{pred},ik} $\n",
        "Сначала найдём производную функции потерь $ L_i $ по предсказанной вероятности $ Y_{\\text{pred},ik} $:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Y_{\\text{pred},ik}} = -\\frac{Y_{ik}}{Y_{\\text{pred},ik}}\n",
        "$$\n",
        "Эта производная возникает напрямую из логарифма: производная от $ \\log(x) $ по $ x $ равна $ \\frac{1}{x} $.\n",
        "\n",
        "### Шаг 2: Производная $ Y_{\\text{pred},ik} $ по $ Z_{il} $\n",
        "Теперь найдём производную $ Y_{\\text{pred},ik} $ по сырым выходам $ Z_{il} $. Здесь нужно учесть, что softmax-функция зависит от всех логитов $ Z_{ij} $, а не только от $ Z_{ik} $, поэтому нужно рассматривать два случая: $ k = l $ и $ k \\neq l $.\n",
        "\n",
        "Для случая $ k = l $:\n",
        "$$\n",
        "\\frac{\\partial Y_{\\text{pred},ik}}{\\partial Z_{ik}} = Y_{\\text{pred},ik} (1 - Y_{\\text{pred},ik})\n",
        "$$\n",
        "Для случая $ k \\neq l $:\n",
        "$$\n",
        "\\frac{\\partial Y_{\\text{pred},ik}}{\\partial Z_{il}} = -Y_{\\text{pred},ik} Y_{\\text{pred},il}\n",
        "$$\n",
        "Эти результаты можно получить, применяя правило дифференцирования для softmax-функции.\n",
        "\n",
        "### Шаг 3: Применение цепного правила\n",
        "Теперь мы можем найти $ \\frac{\\partial L_i}{\\partial Z_{ik}} $, применяя цепное правило:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = \\sum_{l} \\frac{\\partial L_i}{\\partial Y_{\\text{pred},il}} \\cdot \\frac{\\partial Y_{\\text{pred},il}}{\\partial Z_{ik}}\n",
        "$$\n",
        "Подставим производные, которые мы нашли ранее.\n",
        "\n",
        "#### Когда $ k = l $:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = -\\frac{Y_{ik}}{Y_{\\text{pred},ik}} \\cdot Y_{\\text{pred},ik} (1 - Y_{\\text{pred},ik}) = Y_{\\text{pred},ik} - Y_{ik}\n",
        "$$\n",
        "\n",
        "#### Когда $ k \\neq l $:\n",
        "Для $ k \\neq l $ слагаемое равно:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = -\\frac{Y_{il}}{Y_{\\text{pred},il}} \\cdot (-Y_{\\text{pred},ik} Y_{\\text{pred},il}) = 0\n",
        "$$\n",
        "Так как $ Y_{il} = 0 $ для всех $ l \\neq k $ (поскольку в One Hot Encoding метка равна 1 только для правильного класса).\n",
        "\n",
        "### Итог\n",
        "Таким образом, производная функции потерь по $ Z_{ik} $ сводится к:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = Y_{\\text{pred},ik} - Y_{ik}\n",
        "$$\n",
        "Этот результат означает, что для softmax с кросс-энтропийной функцией потерь градиент по логитам (сырым выходам) выражается в виде разности между предсказанной вероятностью и истинной меткой, что существенно упрощает вычисления градиента по весам."
      ],
      "metadata": {
        "id": "t-JIUtWMviMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ваше решение работает, потому что здесь используется метод градиентного спуска для минимизации кросс-энтропийной функции потерь, связанной с softmax-регрессией. В этом методе вычисление градиента функции потерь по весам ($ W $) становится проще, благодаря матричным операциям.\n",
        "\n",
        "Основные шаги:\n",
        "\n",
        "1. **Softmax-функция:** Функция softmax преобразует сырые выходы сети ($ X @ W.T $) в вероятности для каждой метки класса. Она позволяет интерпретировать выходные значения как вероятности принадлежности к классам $ Y=0 $ и $ Y=1 $.\n",
        "\n",
        "2. **Вычисление градиента:** Градиент функции потерь вычисляется как разность предсказанных значений $ Y_{\\text{pred}} $ и реальных меток $ Y $, умноженная на признаки $ X $. В коде это делается с помощью выражения `(Y_pred - Y).T @ X`. Этот подход основан на градиенте кросс-энтропийной функции потерь для softmax-регрессии, где сложные производные упрощаются за счёт умножения разности на матрицу признаков.\n",
        "\n",
        "3. **Обновление весов:** На каждом шаге градиентного спуска веса $ W $ обновляются по формуле `W -= grad_W * h`, где $ h $ — это шаг обучения.\n",
        "\n",
        "Такое обновление весов работает благодаря свойству softmax-функции и кросс-энтропийной функции потерь, при которых градиенты по весам можно выразить в виде, не требующем сложных производных."
      ],
      "metadata": {
        "id": "05W8bkUqvuD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конечно, давайте рассмотрим подробнее, почему градиент кросс-энтропийной функции потерь для softmax-регрессии можно выразить в упрощённой форме.\n",
        "\n",
        "### 1. Кросс-энтропийная функция потерь и softmax\n",
        "Для задачи классификации часто используют кросс-энтропийную функцию потерь, которая измеряет, насколько предсказанные вероятности отклоняются от истинных меток. Пусть:\n",
        "- $ Y $ — истинные метки в формате One Hot Encoding.\n",
        "- $ Y_{\\text{pred}} $ — предсказанные вероятности классов, полученные через softmax.\n",
        "\n",
        "Тогда кросс-энтропийная функция потерь на один объект можно записать как:\n",
        "$$\n",
        "L = -\\sum_{k} Y_k \\log(Y_{\\text{pred},k})\n",
        "$$\n",
        "где суммирование идёт по всем возможным классам $ k $.\n",
        "\n",
        "### 2. Softmax-функция\n",
        "Softmax-функция преобразует сырые выходы линейной модели в вероятности:\n",
        "$$\n",
        "Y_{\\text{pred},i} = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
        "$$\n",
        "где $ z_i $ — линейное преобразование признаков для класса $ i $:\n",
        "$$\n",
        "z_i = W_i \\cdot X\n",
        "$$\n",
        "$ W_i $ — весовые параметры для класса $ i $.\n",
        "\n",
        "### 3. Вычисление градиента\n",
        "Теперь нужно найти градиент функции потерь $ L $ по весам $ W $. Полное дифференцирование кросс-энтропийной функции и softmax-функции может быть сложным, но есть упрощение.\n",
        "\n",
        "#### Производная кросс-энтропии по выходу softmax\n",
        "Для класса $ k $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_k} = Y_{\\text{pred},k} - Y_k\n",
        "$$\n",
        "Это ключевой результат! Производная функции потерь по выходу softmax оказывается просто разностью между предсказанной вероятностью и истинной меткой для каждого класса.\n",
        "\n",
        "#### Производная по весам\n",
        "Теперь, чтобы найти градиент по весам $ W $, воспользуемся цепным правилом:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_{ij}} = \\sum_{k} \\frac{\\partial L}{\\partial z_k} \\cdot \\frac{\\partial z_k}{\\partial W_{ij}}\n",
        "$$\n",
        "Поскольку $ z_k = W_k \\cdot X $, частная производная $ \\frac{\\partial z_k}{\\partial W_{ij}} $ равна $ X_j $ (координата $ j $-го признака для класса $ k $).\n",
        "\n",
        "Таким образом, градиент по весам для $ k $-го класса:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_k} = (Y_{\\text{pred},k} - Y_k) X\n",
        "$$\n",
        "То есть, вклад в градиент для каждого объекта — это просто разность между предсказанной вероятностью и истинной меткой, умноженная на вектор признаков $ X $.\n",
        "\n",
        "### 4. Градиент для всей выборки\n",
        "Для всей выборки (матрица $ X $ из $ m $ объектов) градиент можно записать в виде:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = (Y_{\\text{pred}} - Y)^T X\n",
        "$$\n",
        "где:\n",
        "- $ Y_{\\text{pred}} $ — матрица предсказанных вероятностей (размерностью $ m \\times K $).\n",
        "- $ Y $ — матрица истинных меток (тоже размером $ m \\times K $).\n",
        "\n",
        "### Почему это работает так просто?\n",
        "Секрет простоты этого результата — в особенностях кросс-энтропийной функции потерь в сочетании с softmax. В этом случае градиент сводится к разнице между предсказанием и истинной меткой, умноженной на входные данные. Этот результат:\n",
        "- Эквивалентен \"ручному\" вычислению производных.\n",
        "- Устраняет необходимость в сложных вычислениях, так как матричные операции могут быть реализованы эффективно в виде простых умножений и суммирований.\n",
        "\n",
        "Таким образом, метод матричных операций с разностью $ Y_{\\text{pred}} - Y $ и признаками $ X $ приводит к тому же результату, что и вычисление производных вручную."
      ],
      "metadata": {
        "id": "slVEgJwUvydF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте разберём этот шаг ещё более детально, чтобы стало понятно, как градиент действительно сводится к разнице между предсказанием и истинной меткой, умноженной на входные данные.\n",
        "\n",
        "### Входные данные и обозначения\n",
        "Напомним обозначения:\n",
        "- $ X $ — матрица признаков размерности $ m \\times n $ (где $ m $ — количество объектов, $ n $ — количество признаков).\n",
        "- $ W $ — матрица весов размерности $ K \\times n $ (где $ K $ — количество классов).\n",
        "- $ Y_{\\text{pred}} $ — матрица предсказанных вероятностей размерности $ m \\times K $.\n",
        "- $ Y $ — матрица истинных меток в формате One Hot Encoding, тоже размерности $ m \\times K $.\n",
        "\n",
        "### Вычисление линейного преобразования и softmax\n",
        "Сначала мы вычисляем линейное преобразование:\n",
        "$$\n",
        "Z = X W^T\n",
        "$$\n",
        "где $ Z $ — это матрица размером $ m \\times K $, содержащая сырые выходы для каждого класса.\n",
        "\n",
        "Затем, применяя softmax к каждому выходу, получаем предсказанные вероятности $ Y_{\\text{pred}} $:\n",
        "$$\n",
        "Y_{\\text{pred},i} = \\frac{e^{Z_i}}{\\sum_{j} e^{Z_j}}\n",
        "$$\n",
        "где $ Y_{\\text{pred},i} $ — это предсказанная вероятность для $ i $-го класса.\n",
        "\n",
        "### Кросс-энтропийная функция потерь\n",
        "Кросс-энтропийная функция потерь для всей выборки:\n",
        "$$\n",
        "L = -\\sum_{i=1}^m \\sum_{k=1}^K Y_{ik} \\log(Y_{\\text{pred},ik})\n",
        "$$\n",
        "где $ Y_{ik} $ — истинная метка (0 или 1) для объекта $ i $ и класса $ k $, а $ Y_{\\text{pred},ik} $ — предсказанная вероятность для этого класса.\n",
        "\n",
        "### Шаг 1: Производная функции потерь по $ Z $ (сырым выходам)\n",
        "Чтобы найти градиент, начнём с производной функции потерь по сырым выходам $ Z $, так как они напрямую зависят от весов $ W $.\n",
        "\n",
        "Для класса $ k $ и объекта $ i $ имеем:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial Z_{ik}} = Y_{\\text{pred},ik} - Y_{ik}\n",
        "$$\n",
        "Этот результат означает, что градиент функции потерь по сырым выходам $ Z $ — это разность между предсказанными вероятностями и истинными метками. Почему это так? Потому что производная от $ \\log(Y_{\\text{pred},ik}) $ по $ Z_{ik} $ даёт $ \\frac{1}{Y_{\\text{pred},ik}} \\cdot \\frac{\\partial Y_{\\text{pred},ik}}{\\partial Z_{ik}} $, и при определённых преобразованиях это сводится к $ Y_{\\text{pred},ik} - Y_{ik} $.\n",
        "\n",
        "### Шаг 2: Производная по весам $ W $\n",
        "Теперь мы можем найти градиент функции потерь по весам $ W $. Напомним, что $ Z = X W^T $, поэтому каждый элемент $ Z_{ik} $ зависит от весов $ W_k $, ассоциированных с классом $ k $.\n",
        "\n",
        "Используя цепное правило, получаем:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_{kj}} = \\sum_{i=1}^m \\frac{\\partial L}{\\partial Z_{ik}} \\cdot \\frac{\\partial Z_{ik}}{\\partial W_{kj}}\n",
        "$$\n",
        "\n",
        "Мы уже знаем, что $ \\frac{\\partial L}{\\partial Z_{ik}} = Y_{\\text{pred},ik} - Y_{ik} $. Поскольку $ Z_{ik} = \\sum_{j} W_{kj} X_{ij} $, то $ \\frac{\\partial Z_{ik}}{\\partial W_{kj}} = X_{ij} $.\n",
        "\n",
        "Подставляем:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_{kj}} = \\sum_{i=1}^m (Y_{\\text{pred},ik} - Y_{ik}) X_{ij}\n",
        "$$\n",
        "\n",
        "### Шаг 3: Запись в матричной форме\n",
        "Теперь, если переписать это выражение для всех весов и признаков, то мы можем представить его в виде матричного умножения:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W} = (Y_{\\text{pred}} - Y)^T X\n",
        "$$\n",
        "где:\n",
        "- $ Y_{\\text{pred}} - Y $ — это матрица разностей между предсказанными вероятностями и истинными метками (размерности $ m \\times K $).\n",
        "- $ X $ — матрица признаков размером $ m \\times n $.\n",
        "\n",
        "Это выражение даёт полный градиент функции потерь по всем весам $ W $.\n",
        "\n",
        "### Итог\n",
        "Таким образом, градиент функции потерь по весам $ W $ сводится к умножению разности предсказанных и истинных меток на матрицу признаков $ X $. Этот результат полностью совпадает с тем, что был бы получен при более сложном дифференцировании, поскольку кросс-энтропия с softmax-функцией имеет это удобное свойство, упрощающее вычисления градиентов."
      ],
      "metadata": {
        "id": "1z1sV0bRv2Tb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конечно, давайте подробно разберём, как выводится производная функции потерь по сырым выходам $ Z $, чтобы стало понятно, почему она сводится к $ Y_{\\text{pred},ik} - Y_{ik} $.\n",
        "\n",
        "Для этого воспользуемся несколькими свойствами производной и рассмотрим, как именно происходит это упрощение.\n",
        "\n",
        "### Повторим обозначения\n",
        "- $ Y $ — истинные метки (One Hot Encoding).\n",
        "- $ Y_{\\text{pred}} $ — предсказанные вероятности для каждого класса, полученные через softmax.\n",
        "- $ Z $ — сырые выходы линейного слоя перед применением softmax.\n",
        "\n",
        "Напомним, что:\n",
        "$$\n",
        "Y_{\\text{pred},ik} = \\frac{e^{Z_{ik}}}{\\sum_{j} e^{Z_{ij}}}\n",
        "$$\n",
        "где $ Y_{\\text{pred},ik} $ — это предсказанная вероятность для $ i $-го объекта и $ k $-го класса, а $ Z_{ik} $ — линейное преобразование (логит) для этого объекта и класса.\n",
        "\n",
        "### Кросс-энтропийная функция потерь\n",
        "Для одного объекта с индексом $ i $ кросс-энтропийная функция потерь задаётся как:\n",
        "$$\n",
        "L_i = -\\sum_{k} Y_{ik} \\log(Y_{\\text{pred},ik})\n",
        "$$\n",
        "Наша цель — найти производную этой функции по $ Z_{ik} $.\n",
        "\n",
        "### Шаг 1: Производная потерь по $ Y_{\\text{pred},ik} $\n",
        "Сначала найдём производную функции потерь $ L_i $ по предсказанной вероятности $ Y_{\\text{pred},ik} $:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Y_{\\text{pred},ik}} = -\\frac{Y_{ik}}{Y_{\\text{pred},ik}}\n",
        "$$\n",
        "Эта производная возникает напрямую из логарифма: производная от $ \\log(x) $ по $ x $ равна $ \\frac{1}{x} $.\n",
        "\n",
        "### Шаг 2: Производная $ Y_{\\text{pred},ik} $ по $ Z_{il} $\n",
        "Теперь найдём производную $ Y_{\\text{pred},ik} $ по сырым выходам $ Z_{il} $. Здесь нужно учесть, что softmax-функция зависит от всех логитов $ Z_{ij} $, а не только от $ Z_{ik} $, поэтому нужно рассматривать два случая: $ k = l $ и $ k \\neq l $.\n",
        "\n",
        "Для случая $ k = l $:\n",
        "$$\n",
        "\\frac{\\partial Y_{\\text{pred},ik}}{\\partial Z_{ik}} = Y_{\\text{pred},ik} (1 - Y_{\\text{pred},ik})\n",
        "$$\n",
        "Для случая $ k \\neq l $:\n",
        "$$\n",
        "\\frac{\\partial Y_{\\text{pred},ik}}{\\partial Z_{il}} = -Y_{\\text{pred},ik} Y_{\\text{pred},il}\n",
        "$$\n",
        "Эти результаты можно получить, применяя правило дифференцирования для softmax-функции.\n",
        "\n",
        "### Шаг 3: Применение цепного правила\n",
        "Теперь мы можем найти $ \\frac{\\partial L_i}{\\partial Z_{ik}} $, применяя цепное правило:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = \\sum_{l} \\frac{\\partial L_i}{\\partial Y_{\\text{pred},il}} \\cdot \\frac{\\partial Y_{\\text{pred},il}}{\\partial Z_{ik}}\n",
        "$$\n",
        "Подставим производные, которые мы нашли ранее.\n",
        "\n",
        "#### Когда $ k = l $:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = -\\frac{Y_{ik}}{Y_{\\text{pred},ik}} \\cdot Y_{\\text{pred},ik} (1 - Y_{\\text{pred},ik}) = Y_{\\text{pred},ik} - Y_{ik}\n",
        "$$\n",
        "\n",
        "#### Когда $ k \\neq l $:\n",
        "Для $ k \\neq l $ слагаемое равно:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = -\\frac{Y_{il}}{Y_{\\text{pred},il}} \\cdot (-Y_{\\text{pred},ik} Y_{\\text{pred},il}) = 0\n",
        "$$\n",
        "Так как $ Y_{il} = 0 $ для всех $ l \\neq k $ (поскольку в One Hot Encoding метка равна 1 только для правильного класса).\n",
        "\n",
        "### Итог\n",
        "Таким образом, производная функции потерь по $ Z_{ik} $ сводится к:\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial Z_{ik}} = Y_{\\text{pred},ik} - Y_{ik}\n",
        "$$\n",
        "Этот результат означает, что для softmax с кросс-энтропийной функцией потерь градиент по логитам (сырым выходам) выражается в виде разности между предсказанной вероятностью и истинной меткой, что существенно упрощает вычисления градиента по весам."
      ],
      "metadata": {
        "id": "wMGjYCBKwDc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XEis2sa_rkqv"
      }
    }
  ]
}
