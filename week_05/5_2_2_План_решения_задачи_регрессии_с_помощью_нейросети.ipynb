{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3QxIt8-YsxiF"
      ],
      "authorship_tag": "ABX9TyMgiBGYsqmncQgPYgAs70ZT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Условие"
      ],
      "metadata": {
        "id": "Gu5WICPHYIt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/573081/step/2 [статья 1](https://habr.com/ru/articles/823644/) [статья 2](https://loginom.ru/blog/quality-metrics)\n",
        "\n",
        "Вот вам простенькая нейросеть\n",
        "\n",
        "<img src='https://ucarecdn.com/d40e2fdb-2722-4839-95e8-e90cf679fce5/'>\n",
        "\n",
        "И тренировочная выборка\n",
        "\n",
        "| x | y |\n",
        "|---|---|\n",
        "| -1 | 1 |\n",
        "| 0 | 0 |\n",
        "| 1 | -1 |\n",
        "\n",
        "Для человека не составит труда восстановить зависимость между $x$ и $y$. Ну да, $y=-x$. То есть оптимальные значения весов сети $w_1=-1,w_0=0$. Посмотрим, сумеет ли нейронная сеть обнаружить эти значения в процессе тренировки.\n",
        "\n",
        "Для этого нужно составить функцию потерь $L(w)$, и начать процедуру градиентного спуска (будем применять классический градиентный спуск без всяких наворотов). Для старта спуска возьмем\n",
        "$a_0=(0,0),h=0.1$, причем мы предполагаем, что первая координата позиции \"шарика\" соответствует весу $w_0$, а вторая координата соответствует весу $w_1$.\n",
        "\n",
        "Давайте сделаем два шага градиентного спуска и получим позицию $a_2$. В окошко ответа запишите вторую координату (которая соотвтетствует весу $w_1$) позиции $a_2$.\n",
        "\n",
        "Ну как? Стала ли позиция \"шарика\" $a_2$ ближе к точке истинной минимума функции потерь $(0,-1)$?"
      ],
      "metadata": {
        "id": "rpjDTMwDcD1M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-gXzlnUbqTE"
      },
      "outputs": [],
      "source": [
        "# -0.64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Решение"
      ],
      "metadata": {
        "id": "dtWxs9e3YB1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функция потерь и частные производные\n",
        "- $ N $ — количество наблюдений в выборке,\n",
        "- $ x_i, y_i $ — тренировочные данные,\n",
        "- $ w_0, w_1 $ — значения весов,\n",
        "- $y_{pred} $ — предсказания модели\n",
        "\n",
        "Выход нашей сети $ y_{\\text{pred}} $:\n",
        "\n",
        "$$\n",
        "y_{\\text{pred}} = w_1 \\cdot x + w_0\n",
        "$$\n",
        "\n",
        "Частные производные, понадобятся:\n",
        "\n",
        "$$\\frac{\\partial y_{{pred}}}{\\partial w_0} = 1$$\n",
        "\n",
        "$$\\frac{\\partial y_{{pred}}}{\\partial w_1} = x$$\n",
        "\n",
        "Функция потерь в задаче не задана, пусть будет MSE (традиционно), однако без нормировки на N (нетрадиционно, но нужно для правильного ответа):\n",
        "\n",
        "$$\n",
        "L(w_0, w_1) = \\sum_{i=1}^{N} (y_i - y_{pred})^2\n",
        "$$\n",
        "\n",
        "Здесь тоже берём производную, сначала от внешней функции. Производная функции потерь $ L $ по $ y_{\\text{pred}} $ равна:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial y_{{pred}}} = \\frac{\\partial}{\\partial y_{{pred}}} \\left( \\sum_{i=1}^{N} \\left( y_i - y_{{pred}} \\right)^2 \\right) = \\frac{\\partial L}{\\partial y_{{pred}}} = -2 \\sum_{i=1}^{N} \\left( y_i - y_{{pred}} \\right) = 2 \\sum_{i=1}^{N} \\left( y_{{pred}} - y_i \\right)\n",
        "$$\n",
        "\n",
        "Теперь найдём частные производные $ L(w_0, w_1) $ по весам $ w_0 $ и $ w_1 $ с использованием правила цепочки.\n",
        "\n",
        "Это чуть сложнее, чем раскрыть скобки, но позволит легче перестроиться при замене функции потерь на более сложную.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_0} = \\frac{\\partial L}{\\partial y_{pred}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial w_0} = 2 \\sum_{i=1}^{N} \\left(  y_{pred} - y_i \\right) \\cdot 1\n",
        "$$\n",
        "\n",
        "Аналогично:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{\\text{pred}}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial w_1} = 2 \\sum_{i=1}^{N} \\left(  y_{pred} - y_i \\right) \\cdot x_i\n",
        "$$\n",
        "\n",
        "Подставим $ y_{\\text{pred}} = w_1 \\cdot x_i + w_0 $ в полученные выражения для частных производных:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_0} = 2 \\sum_{i=1}^{N} \\left( (w_1 \\cdot x_i + w_0) - y_i \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1} = 2 \\sum_{i=1}^{N} \\left( (w_1 \\cdot x_i + w_0)- y_i \\right) \\cdot x_i\n",
        "$$\n",
        "\n",
        "Раскрываем сумму, подставив заданные  $ x_i, y_i $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_0} = 2  ((w_1 \\cdot x_1 + w_0) - y_1) + ((w_1 \\cdot x_2 + w_0) - y_2) + ((w_1 \\cdot x_3 + w_0) - y_3)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= 2  ((w_1 \\cdot (-1) + w_0) - 1) + ((w_1 \\cdot 0 + w_0) - 0) + ((w_1 \\cdot 1 + w_0) - (-1)) = 6w_0\n",
        "$$\n",
        "\n",
        "Аналогично\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_0} = 2  ((w_1 \\cdot x_1 + w_0) - y_1) \\cdot x_1 + ((w_1 \\cdot x_2 + w_0) - y_2) \\cdot x_2 + ((w_1 \\cdot x_3 + w_0) - y_3) \\cdot x_3\n",
        "$$\n",
        "\n",
        "$$\n",
        "= 2  ((w_1 \\cdot (-1) + w_0) - 1) \\cdot (-1) + ((w_1 \\cdot 0 + w_0) - 0) \\cdot 0 + ((w_1 \\cdot 1 + w_0) - (-1)) \\cdot 1 = 4w_0 + 4\n",
        "$$"
      ],
      "metadata": {
        "id": "UoTkzUw7sX5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Проверка нахождения производной\n",
        "from sympy import symbols, diff\n",
        "\n",
        "w_0, w_1 = symbols('w_0 w_1')\n",
        "\n",
        "x = [-1, 0, 1]\n",
        "y = [1, 0, -1]\n",
        "\n",
        "y_pred = [w_1 * xi + w_0 for xi in x]\n",
        "L = sum([(pred - yi)**2 for pred, yi in zip(y_pred, y)])\n",
        "\n",
        "display(diff(L, w_0))\n",
        "display(diff(L, w_1))"
      ],
      "metadata": {
        "id": "NnuPkuXvXyw0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 60
        },
        "outputId": "0a574230-b634-476a-f249-1a107aac8af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "6*w_0"
            ],
            "text/latex": "$\\displaystyle 6 w_{0}$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "4*w_1 + 4"
            ],
            "text/latex": "$\\displaystyle 4 w_{1} + 4$"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Градиентный спуск\n",
        "* 1-й шаг градиентного спуска:\n",
        "\n",
        "$w_{0(1)} = w_{0(0)} - h \\cdot \\frac{\\partial L}{\\partial w_0} = w_{0(0)} - h \\cdot 6w_{0(0)} = 0$\n",
        "\n",
        "$w_{1(1)} = w_{1(0)} - h \\cdot \\frac{\\partial L}{\\partial w_1} = w_{1(0)} - h \\cdot (4w_{1(0)} + 4) = -0.4$\n",
        "\n",
        "* 2-й шаг градиентного спуска:\n",
        "\n",
        "$w_{0(2)} = w_{0(1)} - h \\cdot \\frac{\\partial L}{\\partial w_0} = w_{0(1)} - h \\cdot 6w_{0(1)} = 0$\n",
        "\n",
        "$w_{1(2)} = w_{1(1)} - h \\cdot \\frac{\\partial L}{\\partial w_1} = w_{1(1)} - h \\cdot (4w_{1(1)} + 4) = -0.64$"
      ],
      "metadata": {
        "id": "HNZXJaBvdbuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Проверка нахождения весов на 1-м и 2-м шагах\n",
        "der_L_w_0 = lambda w_0: 6 * w_0\n",
        "der_L_w_1 = lambda w_1: 4 * w_1 + 4\n",
        "\n",
        "w_0 = 0\n",
        "w_1 = 0\n",
        "h = 0.1\n",
        "\n",
        "for i in range(2):\n",
        "    w_0 -= h * der_L_w_0(w_0)\n",
        "    w_1 -= h * der_L_w_1(w_1)\n",
        "    print(f'Шаг {i + 1}: w_0 = {w_0}, w_1 = {w_1}')"
      ],
      "metadata": {
        "id": "k0iNdbc1hQKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d85d0b0-bf4b-4e3a-b97c-ac1e1e21e98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Шаг 1: w_0 = 0.0, w_1 = -0.4\n",
            "Шаг 2: w_0 = 0.0, w_1 = -0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Общее решение с возможностью сравнить варианты функции потерь\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([-1, 0, 1])\n",
        "y = np.array([1, 0, -1])\n",
        "\n",
        "def gradients(w0, w1, x, y):\n",
        "    N = len(x)\n",
        "\n",
        "    # dw0 = -2 / N * np.sum(y - (w1 * x + w0)) # версия с нормировкой на N\n",
        "    # dw1 = -2 / N * np.sum(x * (y - (w1 * x + w0)))\n",
        "\n",
        "    dw0 = -2 * np.sum(y - (w1 * x + w0))    # версия без нормировки\n",
        "    dw1 = -2 * np.sum(x * (y - (w1 * x + w0)))\n",
        "\n",
        "    return dw0, dw1\n",
        "\n",
        "# Функция для выполнения градиентного спуска\n",
        "def gradient_descent(w0, w1, x, y, h, steps):\n",
        "    history = [(w0, w1)]  # сохраняем историю весов для каждого шага\n",
        "    for step in range(steps):\n",
        "        dw0, dw1 = gradients(w0, w1, x, y)  # вычисляем градиенты\n",
        "        w0 -= h * dw0  # обновляем веса\n",
        "        w1 -= h * dw1\n",
        "        history.append((w0, w1))  # сохраняем веса после каждого шага\n",
        "    return w0, w1, history\n",
        "\n",
        "# Градиентный спуск на 2 шага с шагом h=0.1\n",
        "steps = 2\n",
        "w0, w1, weights_history = gradient_descent(w0=0, w1=0, x=x, y=y, h=0.1, steps=steps)\n",
        "\n",
        "print(f\"w0 = {w0}, w1 = {w1}\")\n",
        "print(\"История весов:\")\n",
        "for step, (w0_step, w1_step) in enumerate(weights_history):\n",
        "    print(f\"Шаг {step}: w0 = {w0_step}, w1 = {w1_step}\")\n"
      ],
      "metadata": {
        "id": "zXomfEaDWrV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vadim Kopeykin Решение и Пояснение"
      ],
      "metadata": {
        "id": "RQRx8yglXbwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vadim Kopeykin https://stepik.org/lesson/573081/step/2?discussion=8496581&thread=solutions&unit=567630\n",
        "# пояснение https://ucarecdn.com/6b731c42-9306-49ef-bb3d-e845c8116911/\n",
        "import numpy as np\n",
        "\n",
        "w = np.array([0., 0.])\n",
        "X = np.array([[1., -1.],\n",
        "              [1.,  0.],\n",
        "              [1.,  1.]])\n",
        "y = np.array([1., 0., -1.])\n",
        "\n",
        "h = 0.1\n",
        "for i in range(2):\n",
        "    w -= 2 * (X @ w - y) @ X * h\n",
        "    print(w[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHBIiWsVoICh",
        "outputId": "f3c3dc03-cd98-496f-c0c5-c83d457c3eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.4\n",
            "-0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возможны неточности, смотри [рисунок](https://ucarecdn.com/6b731c42-9306-49ef-bb3d-e845c8116911/)\n",
        "\n",
        "У нас есть простой нейрон.\n",
        "\n",
        "Функция нейрона для одного $ k $-ого примера имеет вид:\n",
        "\n",
        "$$\n",
        "y^{(k)}_{pred} = w_0 + x^{(k)} * w_1\n",
        "$$\n",
        "\n",
        "где $ y^{(k)}_{pred} $ — это предсказанное значение нейрона на $ k $-ом примере.\n",
        "\n",
        "Для упрощения выражения добавим единичный вход $ x^{(k)}_0 $, он всегда равен 1:\n",
        "\n",
        "$$\n",
        "y^{(k)}_{pred} = 1 * w_0 + x^{(k)}_1 * w_1 = x^{(k)}_0 * w_0 + x^{(k)}_1 * w_1\n",
        "$$\n",
        "\n",
        "Запишем функцию нейрона в общем виде для одного $ k $-ого примера:\n",
        "\n",
        "$$\n",
        "y^{(k)}_{pred} = \\sum_{i} x^{(k)}_i * w_i \\tag{1}\n",
        "$$\n",
        "\n",
        "где $ i $ — индекс входа и индекс соответствующего веса, $ k $ — индекс примера.\n",
        "\n",
        "Если приглядеться, это выражение можно записать через скалярное произведение.\n",
        "\n",
        "То есть, происходит как бы сворачивание выражения по индексу $ i $.\n",
        "\n",
        "Знак суммы и индекс $ i $ исчезают. А элементы с индексом $ i $ увеличивают свою размерность на 1: числа превращаются в векторы, векторы в матрицы и т. д.:\n",
        "\n",
        "$$\n",
        "y^{(k)}_{pred} = \\overrightarrow{x}^{(k)} * \\overrightarrow{w} \\tag{2}\n",
        "$$\n",
        "\n",
        "Легким движением руки число $ y^{(k)}_{pred} $ превращается в вектор: $ \\overrightarrow{y}_{pred} $,\n",
        "\n",
        "а вектор $ \\overrightarrow{x} $ превращается в матрицу $[X]$:\n",
        "\n",
        "$$\n",
        "\\overrightarrow{y}_{pred} = [X] * \\overrightarrow{w} \\tag{2}\n",
        "$$\n",
        "\n",
        "_______________________________________\n",
        "\n",
        "Давайте найдем производную выхода нейрона по весу, смотрим на выражение (1):\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y^{(k)}_{pred}}{\\partial w_i} = x^{(k)}_i \\tag{3}\n",
        "$$\n",
        "\n",
        "Теперь запишем функцию потерь для одного $ k $-ого примера:\n",
        "\n",
        "$$\n",
        "L^{(k)} = (y^{(k)}_{pred} - y^{(k)})^2\\\n",
        "$$\n",
        "\n",
        "где $ y^{(k)}_{pred} $ — это предсказанное значение нейрона на $ k $-ом примере, а $ y^{(k)} $ — значение $ k $-ого целевого признака.\n",
        "\n",
        "Давайте найдем производную функции потерь по выходу нейрона:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L^{(k)}}{\\partial y^{(k)}_{pred}} = 2 * (y^{(k)}_{pred} - y^{(k)}) \\tag{4}\n",
        "$$\n",
        "\n",
        "Теперь запишем функцию потерь на всех примерах:\n",
        "\n",
        "$$\n",
        "L = \\sum_{k} L^{(k)}\n",
        "$$\n",
        "\n",
        "А теперь давайте найдем частную производную функции потерь на всех примерах по $ i $-му весу.\n",
        "\n",
        "Обратите внимание, что функция потерь — сложная функция:\n",
        "\n",
        "$$\n",
        "L = L(y_{pred}(w_i))\n",
        "$$\n",
        "\n",
        "Поэтому применим правило дифференцирования сложной функции:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial}{\\partial w_i}\\Big[\\sum_{k}L^{(k)}\\Big] = \\sum_{k} \\frac{\\partial L^{(k)}}{\\partial w_i} = \\sum_{k}\\frac{\\partial L}{\\partial y_{pred}^k} * \\frac{\\partial y^k_{pred}}{\\partial w_i}\n",
        "$$\n",
        "\n",
        "То, что я выделил цветом (две дроби под знаком суммы), заменим на выражения (3) и (4) соответственно:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_i} = 2 * \\sum_{k} (y^{(k)}_{pred} - y^{(k)}) * x^{(k)}_i\n",
        "$$\n",
        "________________________________\n",
        "\n",
        "Теперь посмотрите внимательно.\n",
        "\n",
        "Это выражение можно записать как скалярное произведение вектора $ \\overrightarrow{y}_{pred} - \\overrightarrow{y} $ на вектор $ \\overrightarrow{x}_i $.\n",
        "\n",
        "То есть, происходит сворачивание выражения по индексу $ k $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_i} = 2 * (\\overrightarrow{y}_{pred} - \\overrightarrow{y}) \\odot x_i\n",
        "$$\n",
        "\n",
        "А теперь запишем градиент функции потерь **по вектору** весов.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} = 2 * (y_{pred} - y) * [X]^T\n",
        "$$\n",
        "\n",
        "Легким движением руки частная производная $ \\frac{\\partial L}{\\partial w_i} $ превращается в производную по вектору $ \\frac{\\partial L}{\\partial \\overrightarrow{w}} $, а вектор $ \\overrightarrow{x}_i $ превращается в матрицу $[X]$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\overrightarrow{w}} = 2 * (\\overrightarrow{y}_{pred} - \\overrightarrow{y}) \\odot [X]\n",
        "$$\n",
        "\n",
        "Наконец, когда мы научились находить градиент функции потерь **по вектору** весов **на всех** примерах, запишем одну итерацию градиентного спуска:\n",
        "\n",
        "$$\n",
        "\\overrightarrow{w}_{n+1} = \\overrightarrow{w}_n - \\frac{\\partial L}{\\partial \\overrightarrow{w}} * h = \\overrightarrow{w}_n - 2 (\\overrightarrow{y}_{pred} - \\overrightarrow{y} \\odot [X] * h)\n",
        "$$\n",
        "\n",
        "\n",
        "То, что я выделил цветом, заменим на выражение (2), в итоге получаем:\n",
        "\n",
        "$$\n",
        "\\overrightarrow{w}_{n+1} = \\overrightarrow{w}_n - 2 * ([X] \\odot \\overrightarrow{w} - \\overrightarrow{y}) \\odot [X]\n",
        "$$\n",
        "\n",
        "Собственно, это выражение и применяется в коде:\n",
        "\n",
        "```python\n",
        "for i in range(2):\n",
        "    w = 2 * ([X] * w - y) * [X]^T\n",
        "```"
      ],
      "metadata": {
        "id": "iXzthsgdQkZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Форум решений"
      ],
      "metadata": {
        "id": "3QxIt8-YsxiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Игорь Владимирович Лапшин\n",
        "\n",
        "derivative = [lambda x: 6 * x, lambda x: 4 * x + 4] # Производные по w0 и по w1\n",
        "a = [0, 0]\n",
        "number_of_iterations = 2 #Число шагов\n",
        "h = 0.1\n",
        "for n in range(number_of_iterations):\n",
        "    a[0] -= h * derivative[0](a[0])\n",
        "    a[1] -= h * derivative[1](a[1])\n",
        "    print('Шаг: {}    w0 = {}    w1 = {}'.format(1 + n, round(a[0], 3), round(a[1], 3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEB4_Bimq_IF",
        "outputId": "52553127-b1a9-4c21-8fb4-207c9104aff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Шаг: 1    w0 = 0.0    w1 = -0.4\n",
            "Шаг: 2    w0 = 0.0    w1 = -0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Юрий Выборных https://stepik.org/lesson/573081/step/2?discussion=9942318&thread=solutions&unit=567630\n",
        "\n",
        "import numpy as np\n",
        "from sympy import symbols, diff, lambdify\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "w1, w0 = symbols('w1 w0')\n",
        "\n",
        "# Запишем нашу функцию\n",
        "function = (-w1 + w0 - 1) ** 2 + w0 ** 2 + (w1 + w0 + 1) ** 2\n",
        "\n",
        "# Определим производные нашей функции\n",
        "df_w0 = diff(function, w0)\n",
        "display(df_w0)\n",
        "df_w1 = diff(function, w1)\n",
        "display(df_w1)\n",
        "\n",
        "# Зададим первую точку и шаг\n",
        "a0 = (0, 0)    # w0, w1\n",
        "h = 0.1\n",
        "\n",
        "# Находим координаты градиентного спуска\n",
        "point = a0\n",
        "points = (a0,)\n",
        "for _ in range(3):\n",
        "    ldf_w0 = lambdify(w0, expr=df_w0)(w0=point[0])\n",
        "    ldf_w1 = lambdify(w1, expr=df_w1)(w1=point[1])\n",
        "    point = point[0] - h * ldf_w0, point[1] - h * ldf_w1\n",
        "    points += point,\n",
        "\n",
        "\n",
        "print(*points, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "vObjKjLW8dai",
        "outputId": "19972a91-4109-4a65-a5ac-8c19b223c6a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "6*w0"
            ],
            "text/latex": "$\\displaystyle 6 w_{0}$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "4*w1 + 4"
            ],
            "text/latex": "$\\displaystyle 4 w_{1} + 4$"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0)\n",
            "(0.0, -0.4)\n",
            "(0.0, -0.64)\n",
            "(0.0, -0.784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear regression\n",
        "import sympy as sp\n",
        "\n",
        "x_tr_set = [-1, 0, 1]\n",
        "y_tr_set = [1, 0, -1]\n",
        "local_loss = []\n",
        "\n",
        "a = [0, 0]\n",
        "learning_rate = 0.1\n",
        "epochs = 2\n",
        "\n",
        "w1, w0 = sp.symbols('w1, w0', real=True)\n",
        "\n",
        "for x_index, x_value in enumerate(x_tr_set):\n",
        "    local_loss.append((w1 * x_value + w0 - y_tr_set[x_index]) ** 2)\n",
        "\n",
        "loss = sum(local_loss)\n",
        "w1_derivative = loss.diff(w1)\n",
        "w0_derivative = loss.diff(w0)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    a[0] -= learning_rate * w1_derivative.evalf(subs={w1: a[0]})\n",
        "    a[1] -=  learning_rate * w0_derivative.evalf(subs={w0: a[1]})\n",
        "    print(f\"Epoch: {epoch +1}\\t Weight w1: {round(a[0], 3)}\\t Weight w0: {round(a[1], 3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "VoUkoZuylCVr",
        "outputId": "bf73c25a-9e40-410c-e0a4-e178aa26c14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6*w0"
            ],
            "text/latex": "$\\displaystyle 6 w_{0}$"
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "w0, w1 = sp.symbols('w0 w1')\n",
        "L = (-w1+w0-1)**2 + w0**2 + (w1 + w0 + 1)**2\n",
        "\n",
        "dLdw1 = sp.utilities.lambdify(\n",
        "    (w0, w1),\n",
        "    sp.diff(L, w1),\n",
        ")\n",
        "dLdw0 = sp.utilities.lambdify(\n",
        "    (w0, w1),\n",
        "    sp.diff(L, w0),\n",
        ")\n",
        "\n",
        "h = 0.1\n",
        "a_w0 = 0\n",
        "a_w1 = 0\n",
        "n = 10\n",
        "\n",
        "for i in range(0, n + 1):\n",
        "    print(f'a{i}: ({a_w0},  ({a_w1}))')\n",
        "    ai_w0 = a_w0 - h * dLdw0(a_w0, a_w1)\n",
        "    ai_w1 = a_w1 - h * dLdw1(a_w0, a_w1)\n",
        "    a_w0 = ai_w0\n",
        "    a_w1 = ai_w1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reY_r5GdmYkk",
        "outputId": "9aa28d9b-fc9d-4019-b516-e31d90f6febf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a0: (0,  (0))\n",
            "a1: (0.0,  (-0.4))\n",
            "a2: (0.0,  (-0.64))\n",
            "a3: (0.0,  (-0.784))\n",
            "a4: (0.0,  (-0.8704000000000001))\n",
            "a5: (0.0,  (-0.9222400000000001))\n",
            "a6: (0.0,  (-0.9533440000000001))\n",
            "a7: (0.0,  (-0.9720064))\n",
            "a8: (0.0,  (-0.98320384))\n",
            "a9: (0.0,  (-0.989922304))\n",
            "a10: (0.0,  (-0.9939533824))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open AI"
      ],
      "metadata": {
        "id": "erFSuWF3X1_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте начнем с того, что составим функцию потерь для нашей нейронной сети и тренировочной выборки.\n",
        "\n",
        "### Функция потерь\n",
        "\n",
        "Пусть выход нейронной сети $ y_{\\text{pred}} $ задается как:\n",
        "\n",
        "$$\n",
        "y_{\\text{pred}} = w_1 \\cdot x + w_0\n",
        "$$\n",
        "\n",
        "Функция потерь (например, среднеквадратичная ошибка — MSE) на выборке будет выглядеть так:\n",
        "\n",
        "$$\n",
        "L(w_0, w_1) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (w_1 \\cdot x_i + w_0))^2\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $ N $ — количество наблюдений в выборке,\n",
        "- $ x_i, y_i $ — тренировочные данные,\n",
        "- $ w_0, w_1 $ — параметры модели.\n",
        "\n",
        "Теперь выразим градиенты функции потерь по каждому из весов $ w_0 $ и $ w_1 $, чтобы использовать их для градиентного спуска:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_0} = -\\frac{2}{N} \\sum_{i=1}^{N} (y_i - (w_1 \\cdot x_i + w_0))\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1} = -\\frac{2}{N} \\sum_{i=1}^{N} x_i \\cdot (y_i - (w_1 \\cdot x_i + w_0))\n",
        "$$\n",
        "\n",
        "Затем, используя градиентный спуск, обновим веса на каждом шаге:\n",
        "\n",
        "$$\n",
        "w_0^{(t+1)} = w_0^{(t)} - h \\cdot \\frac{\\partial L}{\\partial w_0}\n",
        "$$\n",
        "$$\n",
        "w_1^{(t+1)} = w_1^{(t)} - h \\cdot \\frac{\\partial L}{\\partial w_1}\n",
        "$$\n",
        "\n",
        "Начнем с $ w_0 = 0 $, $ w_1 = 0 $, шаг $ h = 0.1 $ и выполним два шага градиентного спуска. Сейчас я посчитаю это.\n",
        "\n",
        "После двух шагов градиентного спуска значения весов стали:\n",
        "\n",
        "$$\n",
        "w_0 = 0.0, \\quad w_1 \\approx -0.25\n",
        "$$\n",
        "\n",
        "Вес $ w_1 $ начал приближаться к правильному значению $ -1 $, хотя пока значительно меньше по модулю. Параметр $ w_0 $ остался равен нулю, что соответствует точному значению для данной линейной зависимости.\n"
      ],
      "metadata": {
        "id": "0v4LfwyZp_3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([-1, 0, 1])\n",
        "y = np.array([1, 0, -1])\n",
        "\n",
        "def gradients(w0, w1, x, y):\n",
        "    N = len(x)\n",
        "\n",
        "    # dw0 = -2 / N * np.sum(y - (w1 * x + w0)) # версия с нормировкой на N\n",
        "    # dw1 = -2 / N * np.sum(x * (y - (w1 * x + w0)))\n",
        "\n",
        "    dw0 = -2 * np.sum(y - (w1 * x + w0))    # версия без нормировки\n",
        "    dw1 = -2 * np.sum(x * (y - (w1 * x + w0)))\n",
        "\n",
        "    return dw0, dw1\n",
        "\n",
        "# Функция для выполнения градиентного спуска\n",
        "def gradient_descent(w0, w1, x, y, h, steps):\n",
        "    history = [(w0, w1)]  # сохраняем историю весов для каждого шага\n",
        "    for step in range(steps):\n",
        "        dw0, dw1 = gradients(w0, w1, x, y)  # вычисляем градиенты\n",
        "        w0 -= h * dw0  # обновляем веса\n",
        "        w1 -= h * dw1\n",
        "        history.append((w0, w1))  # сохраняем веса после каждого шага\n",
        "    return w0, w1, history\n",
        "\n",
        "# Градиентный спуск на 2 шага с шагом h=0.1\n",
        "steps = 2\n",
        "w0, w1, weights_history = gradient_descent(w0=0, w1=0, x=x, y=y, h=0.1, steps=steps)\n",
        "\n",
        "print(f\"w0 = {w0}, w1 = {w1}\")\n",
        "print(\"История весов:\")\n",
        "for step, (w0_step, w1_step) in enumerate(weights_history):\n",
        "    print(f\"Шаг {step}: w0 = {w0_step}, w1 = {w1_step}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji3jBYFOqIFZ",
        "outputId": "09bcef91-e08d-4c28-d4bd-4ccc4d82af4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w0 = 0.72, w1 = 0.8799999999999999\n",
            "История весов:\n",
            "Шаг 0: w0 = 0, w1 = 0\n",
            "Шаг 1: w0 = 1.2000000000000002, w1 = 1.6\n",
            "Шаг 2: w0 = 0.72, w1 = 0.8799999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Данные\n",
        "x = np.array([-1, 0, 1])\n",
        "y = np.array([1, 0, -1])\n",
        "\n",
        "# Градиенты функции потерь\n",
        "def gradients(w0, w1, x, y):\n",
        "    N = len(x)\n",
        "    predictions = w1 * x + w0\n",
        "    errors = y - predictions\n",
        "    dw0 = -2 * np.sum(errors) / N  # Градиент по w0\n",
        "    dw1 = -2 * np.sum(errors * x) / N  # Градиент по w1\n",
        "    return dw0, dw1\n",
        "\n",
        "# Функция для выполнения градиентного спуска\n",
        "def gradient_descent(w0, w1, x, y, h, steps):\n",
        "    history = [(w0, w1)]  # сохраняем историю весов для каждого шага\n",
        "    for step in range(steps):\n",
        "        dw0, dw1 = gradients(w0, w1, x, y)  # вычисляем градиенты\n",
        "        w0 -= h * dw0  # обновляем w0\n",
        "        w1 -= h * dw1  # обновляем w1\n",
        "        history.append((w0, w1))  # сохраняем веса после каждого шага\n",
        "    return w0, w1, history\n",
        "\n",
        "# Выполним 2 шага градиентного спуска с шагом h=0.1\n",
        "steps = 2\n",
        "w0, w1, weights_history = gradient_descent(w0=0, w1=0, x=x, y=y, h=0.1, steps=steps)\n",
        "\n",
        "# Вывод результатов\n",
        "print(f\"w0 = {w0}, w1 = {w1}\")\n",
        "print(\"История весов:\")\n",
        "for step, (w0_step, w1_step) in enumerate(weights_history):\n",
        "    print(f\"Шаг {step}: w0 = {w0_step}, w1 = {w1_step}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj3DgmmVuQWA",
        "outputId": "c695a8c1-d624-4492-951c-9d556908f34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w0 = 0.0, w1 = -0.2488888888888889\n",
            "История весов:\n",
            "Шаг 0: w0 = 0, w1 = 0\n",
            "Шаг 1: w0 = 0.0, w1 = -0.13333333333333333\n",
            "Шаг 2: w0 = 0.0, w1 = -0.2488888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Тренировочная выборка\n",
        "x_tr_set = [-1, 0, 1]\n",
        "y_tr_set = [1, 0, -1]\n",
        "\n",
        "# Параметры обучения\n",
        "learning_rate = 0.1\n",
        "epochs = 2\n",
        "\n",
        "# Начальные веса\n",
        "w1 = 0.0  # Наклон\n",
        "w0 = 0.0  # Смещение\n",
        "\n",
        "# Градиенты функции потерь\n",
        "def compute_gradients(w0, w1, x, y):\n",
        "    N = len(x)\n",
        "    dw0 = 0.0\n",
        "    dw1 = 0.0\n",
        "\n",
        "    # Вычисляем градиенты\n",
        "    for i in range(N):\n",
        "        prediction = w1 * x[i] + w0\n",
        "        error = prediction - y[i]\n",
        "        dw0 += error  # Градиент по w0\n",
        "        dw1 += error * x[i]  # Градиент по w1\n",
        "\n",
        "    dw0 = (2 / N) * dw0  # Средний градиент по w0\n",
        "    dw1 = (2 / N) * dw1  # Средний градиент по w1\n",
        "\n",
        "    return dw0, dw1\n",
        "\n",
        "# Обучение\n",
        "for epoch in range(epochs):\n",
        "    dw0, dw1 = compute_gradients(w0, w1, x_tr_set, y_tr_set)  # Вычисляем градиенты\n",
        "    w0 -= learning_rate * dw0  # Обновляем w0\n",
        "    w1 -= learning_rate * dw1  # Обновляем w1\n",
        "\n",
        "    print(f\"Epoch: {epoch + 1}\\t Weight w1: {round(w1, 3)}\\t Weight w0: {round(w0, 3)}\")\n",
        "\n",
        "# Вывод итоговых значений весов\n",
        "print(f\"Final Weights: w0 = {round(w0, 3)}, w1 = {round(w1, 3)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIunuwwsvEZ5",
        "outputId": "69bbfcbb-0b2e-44e9-a306-410d688d9e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\t Weight w1: -0.133\t Weight w0: 0.0\n",
            "Epoch: 2\t Weight w1: -0.249\t Weight w0: 0.0\n",
            "Final Weights: w0 = 0.0, w1 = -0.249\n"
          ]
        }
      ]
    }
  ]
}
