{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVAMcOdTa1X1XhYncbDgmM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/573081/step/2 [статья 1](https://habr.com/ru/articles/823644/) [статья 2](https://loginom.ru/blog/quality-metrics)\n",
        "\n",
        "Вот вам простенькая нейросеть\n",
        "\n",
        "<img src='https://ucarecdn.com/d40e2fdb-2722-4839-95e8-e90cf679fce5/'>\n",
        "\n",
        "И тренировочная выборка\n",
        "\n",
        "| x | y |\n",
        "|---|---|\n",
        "| 0 | 1 |\n",
        "| 1 | 2 |\n",
        "| 2 | 3 |\n",
        "\n",
        "Для человека не составит труда восстановить зависимость между $x$ и $y$. Ну да, $y=-x$. То есть оптимальные значения весов сети $w_1=-1,w_0=0$. Посмотрим, сумеет ли нейронная сеть обнаружить эти значения в процессе тренировки.\n",
        "\n",
        "Для этого нужно составить функцию потерь $L(w)$, и начать процедуру градиентного спуска (будем применять классический градиентный спуск без всяких наворотов). Для старта спуска возьмем\n",
        "$a_0=(0,0),h=0.1$, причем мы предполагаем, что первая координата позиции \"шарика\" соответствует весу $w_0$, а вторая координата соответствует весу $w_1$.\n",
        "\n",
        "Давайте сделаем два шага градиентного спуска и получим позицию $a_2$. В окошко ответа запишите первую координату (которая соотвтетствует весу $w_0$) позиции $a_2$.\n",
        "\n",
        "Ну как? Стала ли позиция \"шарика\" $a_2$ ближе к точке истинной минимума функции потерь $(1, 1)$?"
      ],
      "metadata": {
        "id": "eGEBSQlRVvZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgeXEzLgVnQ7",
        "outputId": "ba167630-f3b1-4e0b-e95b-2c5d069b6321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w0 = 0.72, w1 = 0.8799999999999999\n",
            "История весов:\n",
            "Шаг 0: w0 = 0, w1 = 0\n",
            "Шаг 1: w0 = 1.2000000000000002, w1 = 1.6\n",
            "Шаг 2: w0 = 0.72, w1 = 0.8799999999999999\n"
          ]
        }
      ],
      "source": [
        "#@title  Общее решение с возможностью сравнить варианты функции потерь\n",
        "import numpy as np\n",
        "\n",
        "# x = np.array([-1, 0, 1]) # данные предыдущей задачи\n",
        "# y = np.array([1, 0, -1])\n",
        "\n",
        "x = np.array([0, 1, 2])\n",
        "y = np.array([1, 2, 3])\n",
        "\n",
        "# Функция для градиента\n",
        "def gradients(w0, w1, x, y):\n",
        "    N = len(x)\n",
        "\n",
        "    # dw0 = -2 / N * np.sum(y - (w1 * x + w0)) # версия с нормировкой на N\n",
        "    # dw1 = -2 / N * np.sum(x * (y - (w1 * x + w0)))\n",
        "\n",
        "    dw0 = -2 * np.sum(y - (w1 * x + w0))    # версия без нормировки\n",
        "    dw1 = -2 * np.sum(x * (y - (w1 * x + w0)))\n",
        "\n",
        "    return dw0, dw1\n",
        "\n",
        "# Функция для выполнения градиентного спуска\n",
        "def gradient_descent(w0, w1, x, y, h, steps):\n",
        "    history = [(w0, w1)]  # сохраняем историю весов для каждого шага\n",
        "    for step in range(steps):\n",
        "        dw0, dw1 = gradients(w0, w1, x, y)  # вычисляем градиенты\n",
        "        w0 -= h * dw0  # обновляем веса\n",
        "        w1 -= h * dw1\n",
        "        history.append((w0, w1))  # сохраняем веса после каждого шага\n",
        "    return w0, w1, history\n",
        "\n",
        "# Градиентный спуск на 2 шага с шагом h=0.1\n",
        "steps = 2\n",
        "w0, w1, weights_history = gradient_descent(w0=0, w1=0, x=x, y=y, h=0.1, steps=steps)\n",
        "\n",
        "print(f\"w0 = {w0}, w1 = {w1}\")\n",
        "print(\"История весов:\")\n",
        "for step, (w0_step, w1_step) in enumerate(weights_history):\n",
        "    print(f\"Шаг {step}: w0 = {w0_step}, w1 = {w1_step}\")"
      ]
    }
  ]
}
