{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdF0/ecpIrUq87X2E4wgHe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://stepik.org/lesson/573071/step/4\n",
        "\n",
        "[Функция Softmax и ее производная](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)\n",
        "\n",
        "[Softmax для нейронных сетей](https://e2eml.school/softmax)\n",
        "\n",
        "[Логистическая и Softmax-регрессии. Основная идея и реализация с нуля на Python](https://habr.com/ru/articles/803397/)\n",
        "\n",
        "Рассмотрим тренировочную выборку\n",
        "\n",
        "| x | y |\n",
        "|---|---|\n",
        "| -1 | 0 |\n",
        "| 0 | 1 |\n",
        "| 1 | 0 |\n",
        "\n",
        "и будем тренировать нейронную сеть со следующей архитектурой:\n",
        "\n",
        "<img src='https://ucarecdn.com/acad3c70-9b9d-4837-9088-c2dca29dae0f/'>\n",
        "\n",
        "Составьте функцию потерь по заданной тренировочной выборке. Там должно быть три слагаемых (по количеству объектов из тренировочной выборки). Для простоты дифференцирования функцию потерь преобразуйте к виду (тут надо пользоваться свойствами логарифмов и почленным делением дробей):\n",
        "\n",
        "$$L=-ln(...)-ln(...)-ln(...)=ln(1+e...)+ln(1+e...)+ln(1+e...)$$\n",
        "\n",
        "Теперь найдите частные производные по всем переменным функции потерь (пользуясь школьным правилами дифференцирования).\n",
        "\n",
        "Прекрасно. А теперь зададим параметры градиентного спуска. Будем считать, что изначально все веса равны $0$, шаг обучения равен $h=0.1$ Найдите новые значения весов после одного шага градиентного спуска.  В окошко запишите новое значение веса  $w_3$."
      ],
      "metadata": {
        "id": "osU2Mwu3zulH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.05"
      ],
      "metadata": {
        "id": "87fKmW9Oz6iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Решение\n"
      ],
      "metadata": {
        "id": "D9SNCZLQ0ROM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Писать формулы в общем случае — это просто вынос мозга. © Артём Шевляков\n",
        "\n",
        "Полагаю, не я один обратил внимание на контраст между полученными решениями. В большинстве TODO"
      ],
      "metadata": {
        "id": "lYDIUl7p53Py"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4V_UrDgzn48",
        "outputId": "cc053e85-3e76-4dce-d046-bb0541df3a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 1, w1 = 0.0000, w2 = 0.0000, w3 = 0.0500, w4 = -0.0500\n",
            "epoch = 2, w1 = 0.0000, w2 = 0.0000, w3 = 0.0925, w4 = -0.0925\n",
            "epoch = 3, w1 = 0.0000, w2 = 0.0000, w3 = 0.1287, w4 = -0.1287\n"
          ]
        }
      ],
      "source": [
        "#@title softmax\n",
        "from scipy.special import softmax\n",
        "\n",
        "# Функция для вычисления значений до softmax\n",
        "x_raw = lambda x: [w[0] * x + w[2], w[1] * x + w[3]]\n",
        "\n",
        "# Входные данные\n",
        "x = [-1, 0, 1]\n",
        "y = [1, 0, 1]\n",
        "# Начальные значения весов\n",
        "w = [0, 0, 0, 0]\n",
        "# Количество эпох\n",
        "epoches = 3\n",
        "# Шаг обучения\n",
        "h = 0.1\n",
        "\n",
        "# Обучение\n",
        "for epoch in range(epoches):\n",
        "    # Обнуляем градиенты для каждой эпохи\n",
        "    w_grad = [0, 0, 0, 0]\n",
        "    for i in range(len(x)):\n",
        "        # Вычисляем предсказание\n",
        "        y_pred = softmax(x_raw(x[i]))\n",
        "\n",
        "        # Обновляем градиенты для каждого веса\n",
        "        w_grad[0] += (y_pred[0] - y[i]) * x[i]\n",
        "        w_grad[1] += (y_pred[1] - (1-y[i])) * x[i] # (1-y[i]) это One-Hot Encoding\n",
        "        w_grad[2] += (y_pred[0] - y[i])\n",
        "        w_grad[3] += (y_pred[1] - (1-y[i]))\n",
        "\n",
        "    # Обновляем веса\n",
        "    w[0] -= h * w_grad[0]\n",
        "    w[1] -= h * w_grad[1]\n",
        "    w[2] -= h * w_grad[2]\n",
        "    w[3] -= h * w_grad[3]\n",
        "\n",
        "    # Выводим веса на каждой эпохе\n",
        "    print(f'epoch = {epoch+1}, w1 = {w[0]:.4f}, w2 = {w[1]:.4f}, w3 = {w[2]:.4f}, w4 = {w[3]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Форум"
      ],
      "metadata": {
        "id": "x-vFoDXTXepJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Адамович Александр https://stepik.org/lesson/573071/step/5?discussion=7858120&unit=567620\n",
        "#Классификация для НС\n",
        "import sympy as sp\n",
        "\n",
        "# Объявляем переменные\n",
        "w1, w2, w3, w4 = sp.symbols('w1 w2 w3 w4 ')\n",
        "\n",
        "#Вид функции потерь\n",
        "L1 = - sp.ln(sp.exp(-w1+w3)/(sp.exp(-w1+w3) + sp.exp(-w2+w4)))\n",
        "L2 = - sp.ln(sp.exp(w4)/(sp.exp(w3) + sp.exp(w4)))\n",
        "L3 = - sp.ln(sp.exp(w1+w3)/(sp.exp(w1+w3) + sp.exp(w2+w4)))\n",
        "L = L1 + L2 + L3\n",
        "\n",
        "# Производная L по w1, w2, w3, w4\n",
        "dLdw1 = sp.lambdify((w1, w2, w3, w4), sp.diff(L, w1))\n",
        "dLdw2 = sp.lambdify((w1, w2, w3, w4), sp.diff(L, w2))\n",
        "dLdw3 = sp.lambdify((w1, w2, w3, w4), sp.diff(L, w3))\n",
        "dLdw4 = sp.lambdify((w1, w2, w3, w4), sp.diff(L, w4))\n",
        "\n",
        "# Исходные значения для ГС\n",
        "a = [0, 0, 0, 0]\n",
        "h = 0.1\n",
        "\n",
        "for i in range(1, 3):\n",
        "    x1, x2, x3, x4 = a\n",
        "    a[0] -= h*dLdw1(x1, x2, x3, x4)\n",
        "    a[1] -= h*dLdw2(x1, x2, x3, x4)\n",
        "    a[2] -= h*dLdw3(x1, x2, x3, x4)\n",
        "    a[3] -= h*dLdw4(x1, x2, x3, x4)\n",
        "    print('a[', i,']=[', a[0], ',', a[1],',', a[2], ',', a[3], ']', sep='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9UtkHj1Xk0P",
        "outputId": "d12eb0bc-32b7-46ce-bb8e-32bffeebba0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a[1]=[0.0,0.0,0.05,-0.05]\n",
            "a[2]=[0.0,0.0,0.09250624375631802,-0.092506243756318]\n"
          ]
        }
      ]
    }
  ]
}